
\documentclass[12pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{tocloft}  % Per personalizzazione indice


\geometry{
    a4paper,
    left=2.5cm,
    right=2.5cm,
    top=3cm,
    bottom=3cm
}

% Impostazioni header e footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{\thepage}
\fancyfoot[C]{Progetto Multimedia e Laboratorio}

% Profondità dell'indice
\setcounter{tocdepth}{3}  % Mostra fino a subsubsection
\setcounter{secnumdepth}{3}  % Numera fino a subsubsection

% Personalizzazione indice
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}  % Puntini anche per le sezioni
\renewcommand{\cftsubsecleader}{\cftdotfill{\cftdotsep}}  % Puntini per subsection

% Configurazione listings per codice Python
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!60!black}\itshape,
    stringstyle=\color{red},
    showstringspaces=false,
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=8pt,
    backgroundcolor=\color{gray!10},
    frame=single,
    rulecolor=\color{black},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=4,
    captionpos=b
}

% Configurazione hyperref
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=green,
    pdftitle={Relazione Progetto Multimedia},
    pdfauthor={Studente},
    pdfsubject={Tracciamento Oggetti e Analisi Qualità Video},
    pdfkeywords={tracking, video processing, metriche qualità, OpenCV}
}

% ============================================================================
% INIZIO DOCUMENTO
% ============================================================================

\begin{document}

% ============================================================================
% FRONTESPIZIO
% ============================================================================

\begin{titlepage}
    \centering
    \vspace*{2cm}
    
    \includegraphics[width=0.7\textwidth]{immagini/Logo_UniCT.png}
    \vspace{1cm}
    
    {\Huge\bfseries Università degli Studi di Catania\par}
    \vspace{0.5cm}
    {\Large Corso di Laurea Magistrale in Informatica\par}
    \vspace{2cm}
    
    {\huge\bfseries Sistema di Tracciamento Oggetti in Video\\con Analisi mediante\\Metriche Full-Reference\par}
    \vspace{1.5cm}
    
    {\Large Progetto di Multimedia e Laboratorio\par}
    \vspace{2cm}
    
    \begin{minipage}[t]{0.4\textwidth}
        \begin{flushleft}
            {\large\textbf{Studente:}\\
            Michele Guglielmino\\
            Matricola:1000031551}
        \end{flushleft}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.4\textwidth}
        \begin{flushright}
            {\large\textbf{Docente:}\\
            Prof. Dario Allegra}
        \end{flushright}
    \end{minipage}
    
    \vfill
    
    {\large Anno Accademico 2025/2026\par}
\end{titlepage}

% ============================================================================
% INDICE
% ============================================================================

\clearpage
\pdfbookmark[1]{Indice}{toc}  % Segnalibro PDF per l'indice
{
    \hypersetup{linkcolor=blue}  % Link blu nell'indice
    \tableofcontents
}
\clearpage

% ============================================================================
% ABSTRACT
% ============================================================================

\begin{abstract}
Il presente progetto si inserisce nell'ambito dell'elaborazione e analisi di contenuti multimediali, concentrandosi specificamente sul tracciamento automatico di oggetti in movimento all'interno di sequenze video e sulla valutazione quantitativa della qualità delle trasformazioni applicate. L'obiettivo principale consiste nello sviluppo di un sistema integrato capace di identificare, seguire e contrassegnare entità dinamiche presenti nei fotogrammi mediante l'impiego di algoritmi di \textit{background subtraction} basati su modelli statistici, e di valutare oggettivamente l'impatto di diverse operazioni di post-processing attraverso l'applicazione di metriche \textit{full-reference}.

Il sistema implementato si articola in quattro moduli funzionali principali: un modulo di orchestrazione del flusso video (\texttt{main.py}), un algoritmo di tracciamento basato su distanza euclidea (\texttt{tracciamento.py}), un insieme di filtri di post-processing (\texttt{filtri.py}), e un modulo di calcolo metriche qualitative (\texttt{Calcolo\_metriche.py}). L'architettura modulare adottata consente una chiara separazione delle responsabilità e facilita l'estensibilità del sistema.

La metodologia di tracciamento si basa sull'algoritmo MOG2 (\textit{Mixture of Gaussians 2}) per la segmentazione dinamica dello sfondo, seguito da un'associazione temporale degli oggetti rilevati mediante calcolo della distanza euclidea tra i centroidi. Le metriche di qualità implementate includono MSE (\textit{Mean Squared Error}), PSNR (\textit{Peak Signal-to-Noise Ratio}) e SSIM (\textit{Structural Similarity Index}), calcolate confrontando l'intera sequenza video originale con le sequenze risultanti dall'applicazione di tre differenti algoritmi di elaborazione: compressione JPEG lossy, sfocatura gaussiana e sharpening mediante unsharp masking.

I risultati ottenuti dimostrano l'efficacia del sistema nel tracciamento accurato di oggetti multipli in scenari dinamici, nonché la capacità delle metriche implementate di quantificare oggettivamente le alterazioni introdotte dai diversi algoritmi di elaborazione. L'analisi comparativa mostra come algoritmi di smoothing conservino maggiormente la similarità strutturale rispetto a trasformazioni aggressive come la compressione lossy, e come lo sharpening possa migliorare la nitidezza percepita pur alterando il segnale originale.
\end{abstract}

\newpage

% ============================================================================
% CAPITOLO 1: INTRODUZIONE
% ============================================================================

\section{Introduzione}

\subsection{Contesto e Motivazioni}

L'elaborazione di contenuti video rappresenta uno dei pilastri fondamentali dell'informatica moderna, con applicazioni che spaziano dalla sorveglianza automatizzata alla realtà aumentata, dall'analisi sportiva ai sistemi avanzati di assistenza alla guida. Nel contesto dell'era digitale, la capacità di estrarre informazioni semantiche ad alto livello da sequenze di fotogrammi costituisce una competenza essenziale per numerosi domini applicativi.

Il movimento rappresenta una caratteristica intrinseca e distintiva dei contenuti video, differenziandoli dalle immagini statiche. La capacità di rilevare, tracciare e analizzare entità in movimento fornisce informazioni preziose relative alla dinamica della scena, consentendo di implementare sistemi intelligenti capaci di reagire agli eventi osservati. Applicazioni pratiche includono il monitoraggio del traffico veicolare, il riconoscimento di azioni umane, la videosorveglianza intelligente, e l'analisi comportamentale.

Parallelamente, la valutazione oggettiva della qualità video assume rilevanza crescente in contesti dove le trasformazioni applicate ai contenuti multimediali devono essere quantificate e validate. Operazioni di filtraggio, compressione, restauro e miglioramento possono introdurre degradazioni percettive che necessitano di essere misurate mediante metriche standardizzate. Le metriche \textit{full-reference} permettono confronti diretti tra contenuti originali e trasformati, fornendo indicatori numerici della fedeltà e della similarità strutturale.

\subsection{Obiettivi del Progetto}

Il progetto persegue i seguenti obiettivi principali:

\begin{enumerate}
    \item \textbf{Implementazione di un sistema di tracciamento robusto}: sviluppare un algoritmo capace di identificare e seguire automaticamente oggetti multipli in movimento attraverso sequenze video, assegnando identificatori univoci e persistenti nel tempo.
    
    \item \textbf{Applicazione di trasformazioni di post-processing}: implementare una collezione di filtri rappresentativi delle principali categorie di operazioni di elaborazione video.
    
    \item \textbf{Valutazione quantitativa mediante metriche oggettive}: calcolare metriche standard (MSE, PSNR, SSIM) per confrontare sistematicamente le sequenze originali con quelle trasformate, quantificando l'impatto di ciascun filtro applicato.
    
    \item \textbf{Sperimentazione su contenuti reali}: applicare il sistema sviluppato a video realistici contenenti oggetti in movimento, validando l'efficacia delle tecniche implementate in scenari pratici.
\end{enumerate}

% ============================================================================
% CAPITOLO 2: RIFERIMENTI TEORICI
% ============================================================================

\section{Riferimenti Teorici}

\subsection{Il Movimento nei Video}

I video digitali rappresentano sequenze temporali di immagini bidimensionali (fotogrammi) acquisite a frequenze standard (tipicamente 24, 25, 30 o 60 fotogrammi al secondo). Il movimento percepito dal sistema visivo umano durante la riproduzione video emerge dalla rapida successione di fotogrammi leggermente differenti, fenomeno noto come persistenza retinica.

\subsubsection{Rappresentazione del Movimento}

Quando si analizza il movimento in ambito computazionale, è necessario distinguere tra movimento tridimensionale reale nella scena e movimento bidimensionale proiettato sul piano immagine. La proiezione prospettica di un punto $P = (X, Y, Z)$ nello spazio tridimensionale sul piano immagine con coordinate $(x, y)$ è governata dalle relazioni:

\begin{equation}
x = F \frac{X}{Z}, \quad y = F \frac{Y}{Y}
\end{equation}

dove $F$ rappresenta la lunghezza focale della telecamera e $Z$ la profondità del punto rispetto al centro ottico.

La differenza tra la posizione di un punto $\mathbf{x}$ nel fotogramma al tempo $t$ e la sua posizione nel fotogramma al tempo $t+\Delta t$ definisce il \textbf{vettore di movimento} (motion vector):

\begin{equation}
\mathbf{d}(\mathbf{x}) = \mathbf{x}_{t+\Delta t} - \mathbf{x}_t
\end{equation}

L'insieme dei vettori di movimento per tutti i punti dell'immagine costituisce il \textbf{campo di movimento} (motion field). In pratica, si adottano rappresentazioni semplificate del campo di movimento:

\begin{itemize}
    \item \textbf{Rappresentazione basata su blocchi}: l'immagine viene suddivisa in blocchi regolari (tipicamente $8\times8$ o $16\times16$ pixel) e si assume che tutti i pixel di un blocco condividano lo stesso vettore di movimento.
    \item \textbf{Rappresentazione basata su regioni}: si segmenta l'immagine in regioni semanticamente omogenee e si assegna un vettore di movimento per regione.
    \item \textbf{Rappresentazione basata su feature}: si individuano punti salienti (corner, edge) e si traccia il loro movimento.
\end{itemize}

\subsubsection{Background Subtraction e MOG2}

La sottrazione di sfondo rappresenta una tecnica fondamentale per l'identificazione di oggetti in movimento. Il principio consiste nel confrontare ciascun fotogramma $I_t(\mathbf{x})$ con un modello dello sfondo statico $B(\mathbf{x})$, identificando come foreground i pixel che si discostano significativamente dal modello:

\begin{equation}
F_t(\mathbf{x}) = \begin{cases}
1 & \text{se } |I_t(\mathbf{x}) - B(\mathbf{x})| > \tau \\
0 & \text{altrimenti}
\end{cases}
\end{equation}

dove $\tau$ è una soglia predefinita e $F_t(\mathbf{x})$ è la maschera binaria di foreground.

L'algoritmo \textbf{MOG2} (Mixture of Gaussians 2) rappresenta un'evoluzione dell'approccio classico MOG, introducendo un numero adattivo di distribuzioni gaussiane per modellare ciascun pixel dello sfondo. Ogni pixel viene modellato mediante una miscela di $K$ distribuzioni gaussiane:

\begin{equation}
P(I_t(\mathbf{x})) = \sum_{k=1}^{K} w_{k,t}(\mathbf{x}) \cdot \mathcal{N}(I_t(\mathbf{x}) \mid \mu_{k,t}(\mathbf{x}), \sigma_{k,t}^2(\mathbf{x}))
\end{equation}

dove $w_{k,t}$ sono i pesi della miscela, $\mu_{k,t}$ le medie e $\sigma_{k,t}^2$ le varianze. I parametri vengono aggiornati dinamicamente mediante un processo di apprendimento online che incorpora ogni nuovo fotogramma osservato.

MOG2 introduce inoltre un criterio di selezione automatica del numero ottimale di gaussiane per ciascun pixel, migliorando l'adattabilità a variazioni di illuminazione e riducendo i falsi positivi causati da ombre e riflessi.

\subsection{Tracciamento Oggetti}

Una volta individuate le regioni di foreground mediante background subtraction, è necessario associare tali regioni attraverso i fotogrammi successivi per costruire traiettorie temporali coerenti. Il tracciamento multi-oggetto richiede la risoluzione del problema di \textit{data association}: dato un insieme di rilevamenti al tempo $t$ e un insieme di tracce esistenti, determinare quali rilevamenti corrispondono a quali tracce.

\subsubsection{Tracciamento basato su Distanza Euclidea}

L'approccio più semplice per il tracciamento consiste nel calcolare la distanza euclidea tra i centroidi degli oggetti rilevati in fotogrammi consecutivi. Dato un oggetto con centroide $\mathbf{c}_t = (x_t, y_t)$ al tempo $t$ e un insieme di rilevamenti con centroidi $\{\mathbf{r}_i\}$ al tempo $t+1$, si associa l'oggetto al rilevamento più vicino:

\begin{equation}
\mathbf{r}^* = \arg\min_{\mathbf{r}_i} \|\mathbf{c}_t - \mathbf{r}_i\|_2
\end{equation}

dove la norma euclidea è definita come:

\begin{equation}
\|\mathbf{c}_t - \mathbf{r}_i\|_2 = \sqrt{(x_t - x_i)^2 + (y_t - y_i)^2}
\end{equation}

L'associazione viene accettata solo se la distanza è inferiore a una soglia predefinita $d_{\text{max}}$, garantendo che non vengano associate entità troppo distanti spazialmente.

\subsubsection{Feature-based Tracking con ORB}

\textbf{ORB (Oriented FAST and Rotated BRIEF)} rappresenta un algoritmo di feature detection e description efficiente e robusto. ORB combina il detector FAST (Features from Accelerated Segment Test) per l'identificazione di keypoints con il descriptor BRIEF (Binary Robust Independent Elementary Features) orientato per garantire invarianza alla rotazione.

Il feature matching consente di associare oggetti basandosi non solo sulla posizione geometrica, ma anche su caratteristiche visuali locali (bordi, texture, pattern). Dati due oggetti $O_t$ e $O_{t+1}$ rilevati in fotogrammi consecutivi, si estraggono feature ORB dalle rispettive regioni e si calcolano le corrispondenze mediante \textbf{BFMatcher (Brute Force Matcher)} con distanza Hamming.

Lo score combinato di similarità integra:
\begin{itemize}
    \item \textbf{Similarità feature}: numero di feature corrispondenti (threshold distanza Hamming minore di 50)
    \item \textbf{Distanza geometrica}: distanza euclidea tra centroidi normalizzata
\end{itemize}

\begin{equation}
S_{\text{match}} = 0.7 \cdot \frac{N_{\text{matches}}}{\max(N_{kp1}, N_{kp2})} + 0.3 \cdot \left(1 - \frac{d_{\text{eucl}}}{d_{\text{max}}}\right)
\end{equation}

dove $N_{\text{matches}}$ è il numero di feature corrispondenti, $N_{kp1}, N_{kp2}$ sono i numeri di keypoints estratti dai due oggetti, $d_{\text{eucl}}$ è la distanza euclidea tra centroidi e $d_{\text{max}}$ è la soglia massima configurabile.

\subsection{Stabilizzazione Video}

La stabilizzazione video mira a compensare i movimenti indesiderati della camera (jitter, shake) che causano spostamenti globali dell'intera scena. Questi movimenti globali generano falsi positivi nella background subtraction, poiché pixel statici della scena appaiono in movimento relativo al frame di riferimento.

\subsubsection{Stabilizzazione Feature-based}

L'approccio feature-based stima la trasformazione geometrica tra fotogrammi consecutivi mediante:

\begin{enumerate}
    \item \textbf{Estrazione feature}: identificazione di keypoints salienti (ORB detector con 2000 features)
    \item \textbf{Feature matching}: corrispondenza di feature tra frame corrente e frame di riferimento (primo frame del video)
    \item \textbf{Stima trasformazione}: calcolo della trasformazione affine ottimale mediante RANSAC
    \item \textbf{Smoothing}: applicazione di moving average sulle trasformazioni per evitare movimenti bruschi
    \item \textbf{Warping}: applicazione della trasformazione per allineare il frame corrente al riferimento
\end{enumerate}

\subsubsection{Trasformazione Affine}

Una trasformazione affine 2D preserva parallelismo e rapporti di distanze lungo linee parallele, ed è definita da 6 parametri:

\begin{equation}
\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} + \begin{bmatrix} t_x \\ t_y \end{bmatrix}
\end{equation}

La matrice $2 \times 3$ parametrizza rotazione, scaling, shearing e traslazione. L'algoritmo \texttt{estimateAffinePartial2D} di OpenCV stima i parametri ottimali minimizzando l'errore di riproiezione delle feature corrispondenti, utilizzando RANSAC per robustezza agli outliers.

\subsection{Operazioni Morfologiche}

Le operazioni morfologiche matematiche operano su immagini binarie modificando la forma delle regioni mediante operazioni di set theory con elementi strutturanti (kernel).

\subsubsection{Closing (Chiusura)}

L'operazione di closing è definita come dilatazione seguita da erosione:

\begin{equation}
A \bullet B = (A \oplus B) \ominus B
\end{equation}

dove $A$ è l'immagine binaria, $B$ è l'elemento strutturante, $\oplus$ denota dilatazione e $\ominus$ erosione. Il closing \textbf{riempie piccoli buchi} e \textbf{connette regioni vicine}, utile per fondere detection frammentate dello stesso oggetto (es. tetto, cofano, finestrini di un'auto).

Nel progetto si utilizza un kernel ellittico $7 \times 7$ applicato con 2 iterazioni per garantire fusione efficace di regioni adiacenti.

\subsubsection{Opening (Apertura)}

L'opening è definito come erosione seguita da dilatazione:

\begin{equation}
A \circ B = (A \ominus B) \oplus B
\end{equation}

L'opening \textbf{rimuove piccole regioni isolate} (rumore) preservando le regioni più grandi. Viene applicato dopo il closing (1 iterazione) per eliminare falsi positivi residui senza degradare le detection valide.

\subsection{Non-Maximum Suppression (NMS)}

Il Non-Maximum Suppression elimina detection duplicate dello stesso oggetto selezionando la bounding box più rappresentativa tra quelle sovrapposte. La sovrapposizione è quantificata mediante \textbf{Intersection over Union (IoU)}:

\begin{equation}
\text{IoU}(B_1, B_2) = \frac{\text{Area}(B_1 \cap B_2)}{\text{Area}(B_1 \cup B_2)}
\end{equation}

dove $B_1, B_2$ sono due bounding box. L'algoritmo NMS implementato procede:

\begin{enumerate}
    \item Ordinamento delle detection per area decrescente
    \item Selezione della detection con area massima
    \item Rimozione di tutte le detection con $\text{IoU} > \tau$ rispetto alla detection selezionata
    \item Iterazione fino ad esaurimento delle detection
\end{enumerate}

Il threshold IoU configurato è $\tau = 0.3$, bilanciando eliminazione di duplicati e preservazione di oggetti distinti parzialmente sovrapposti.

\subsection{Coordinate Smoothing}

Il coordinate smoothing riduce il jitter (instabilità) delle bounding box che può derivare da piccole variazioni nel rilevamento dei contorni frame-by-frame. Si applica un \textbf{exponential moving average (EMA)} sulle coordinate del centroide:

\begin{equation}
\mathbf{c}_t^{\text{smooth}} = \alpha \cdot \mathbf{c}_t + (1 - \alpha) \cdot \mathbf{c}_{t-1}^{\text{smooth}}
\end{equation}

dove $\alpha = 0.3$ è il fattore di smoothing. Valori bassi di $\alpha$ producono traiettorie più fluide ma con maggiore latency; $\alpha = 0.3$ bilancia reattività e stabilità.

\subsection{Metriche di Qualità per Video}

La valutazione della qualità video può essere condotta secondo approcci soggettivi o oggettivi. Le metriche soggettive (MOS - Mean Opinion Score, DSCQS - Double Stimulus Continuous Quality Scale) richiedono la partecipazione di osservatori umani e risultano onerose in termini di tempo e costi. Le metriche oggettive automatizzano il processo mediante formule matematiche.

Le metriche oggettive si classificano in base alla disponibilità del segnale di riferimento:

\begin{itemize}
    \item \textbf{Full-Reference (FR)}: richiedono la disponibilità completa del segnale originale non distorto.
    \item \textbf{Reduced-Reference (RR)}: utilizzano informazioni parziali estratte dal segnale originale.
    \item \textbf{No-Reference (NR)}: operano senza alcuna informazione sul segnale originale.
\end{itemize}

Nel presente progetto si adottano metriche full-reference, confrontando fotogramma per fotogramma le sequenze originali con quelle trasformate.

\subsubsection{Mean Squared Error (MSE)}

L'errore quadratico medio quantifica la divergenza media tra i valori dei pixel di due immagini. Date due immagini $I_1$ e $I_2$ di dimensioni $M \times N$, l'MSE è definito come:

\begin{equation}
\text{MSE}(I_1, I_2) = \frac{1}{MN} \sum_{i=1}^{M} \sum_{j=1}^{N} \left[I_1(i,j) - I_2(i,j)\right]^2
\end{equation}

Per sequenze video, si calcola l'MSE medio su tutti i fotogrammi corrispondenti:

\begin{equation}
\text{MSE}_{\text{video}} = \frac{1}{T} \sum_{t=1}^{T} \text{MSE}(I_{\text{orig}}^{(t)}, I_{\text{filt}}^{(t)})
\end{equation}

dove $T$ è il numero totale di fotogrammi. Valori bassi di MSE indicano elevata similarità.

\subsubsection{Peak Signal-to-Noise Ratio (PSNR)}

Il PSNR esprime la qualità in termini logaritmici del rapporto tra il segnale massimo possibile e il rumore di distorsione, fornendo una metrica in decibel:

\begin{equation}
\text{PSNR}(I_1, I_2) = 10 \log_{10} \left( \frac{\text{MAX}_I^2}{\text{MSE}(I_1, I_2)} \right) = 20 \log_{10} \left( \frac{\text{MAX}_I}{\sqrt{\text{MSE}(I_1, I_2)}} \right)
\end{equation}

dove $\text{MAX}_I$ rappresenta il valore massimo assumibile dai pixel (255 per immagini a 8 bit). Maggiore è il suo valore maggiore sarà la “somiglianza” con l'originale.

\subsubsection{Structural Similarity Index (SSIM)}

L'SSIM supera le limitazioni di MSE e PSNR considerando la percezione umana della similarità strutturale. L'ipotesi fondamentale è che il sistema visivo umano sia particolarmente sensibile alle informazioni strutturali dell'immagine.

Date due finestre locali $\mathbf{x}$ e $\mathbf{y}$ estratte dalle immagini da confrontare, l'SSIM è definito come:

\begin{equation}
\text{SSIM}(\mathbf{x}, \mathbf{y}) = \frac{(2\mu_x\mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}
\end{equation}

dove:
\begin{itemize}
    \item $\mu_x, \mu_y$ sono le medie locali
    \item $\sigma_x^2, \sigma_y^2$ sono le varianze locali
    \item $\sigma_{xy}$ è la covarianza
    \item $C_1 = (K_1 L)^2, C_2 = (K_2 L)^2$ sono costanti di stabilizzazione ($K_1 = 0.01, K_2 = 0.03, L = 255$ per immagini a 8 bit)
\end{itemize}

L'indice SSIM assume valori nell'intervallo $[-1, 1]$, dove 1 indica identità perfetta e -1 massima dissimilarità. Per l'intera immagine, si calcola la media degli SSIM locali calcolati su finestre scorrevoli.

% ============================================================================
% CAPITOLO 3: ARCHITETTURA DEL SISTEMA
% ============================================================================

\section{Architettura del Sistema}

\subsection{Panoramica dell'Architettura}

Il sistema implementato adotta un'architettura modulare organizzata in quattro componenti principali, ciascuno responsabile di funzionalità specifiche:

\begin{enumerate}
    \item \textbf{Modulo di orchestrazione} (\texttt{main.py}): coordina il flusso di esecuzione, gestisce l'acquisizione video, la selezione della ROI, implementa la \textbf{dual-pipeline architecture} (tracciamento parallelo su frame originali e frame stabilizzati+filtrati), applica stabilizzazione video, operazioni morfologiche, NMS, invoca gli algoritmi di tracking e filtraggio, e presenta i risultati.
    
    \item \textbf{Modulo di tracciamento} (\texttt{tracciamento.py}): implementa la classe \texttt{Tracciamento} che gestisce l'assegnazione di identificatori univoci agli oggetti rilevati mediante \textbf{ORB feature matching}, calcolo score combinato (feature similarity + distanza euclidea), \textbf{coordinate smoothing} (exponential moving average), e \textbf{NMS con IoU} per eliminazione detection duplicate.
    
    \item \textbf{Modulo di post-processing e stabilizzazione} (\texttt{filtri.py}): fornisce la classe \texttt{VideoStabilizer} per stabilizzazione feature-based mediante ORB (2000 features), trasformazione affine e smoothing, oltre alle funzioni per algoritmi di qualità ai fotogrammi (compressione JPEG, gaussian blurring, unsharp masking) e moving average spaziale.
    
    \item \textbf{Modulo di valutazione} (\texttt{Calcolo\_metriche.py}): implementa le metriche di qualità full-reference per il confronto quantitativo tra sequenze video.
\end{enumerate}

La separazione modulare favorisce la mantenibilità, il testing e l'estensibilità del codice. Ciascun modulo può essere sviluppato, testato e modificato indipendentemente dagli altri, purché vengano rispettate le interfacce definite.

\subsection{Dual-Pipeline Architecture}

Il sistema implementa una \textbf{architettura a due pipeline parallele} per confrontare l'efficacia della stabilizzazione e del filtering:

\begin{enumerate}
    \item \textbf{Pipeline Baseline}: elabora i frame originali senza preprocessing
    \begin{itemize}
        \item Frame originale $\rightarrow$ MOG2 $\rightarrow$ Contour detection $\rightarrow$ Filtering (area) $\rightarrow$ Tracking
    \end{itemize}
    
    \item \textbf{Pipeline Stabilized+Filtered}: applica preprocessing avanzato prima del tracking
    \begin{itemize}
        \item Frame originale $\rightarrow$ \textbf{Video Stabilization} $\rightarrow$ \textbf{Moving Average Spatial} $\rightarrow$ MOG2 $\rightarrow$ \textbf{Morphological Operations} $\rightarrow$ Contour detection $\rightarrow$ \textbf{NMS} $\rightarrow$ Tracking
    \end{itemize}
\end{enumerate}

Entrambe le pipeline operano sullo stesso video in parallelo, consentendo confronto visivo diretto tra i risultati. La pipeline stabilized+filtered riduce significativamente i falsi positivi dovuti a movimento camera e frammentazione oggetti.

\subsection{Pipeline Principale}

Il file \texttt{main.py} implementa la dual-pipeline di elaborazione video, orchestrando l'interazione tra i vari moduli. Il flusso di esecuzione si articola nelle seguenti fasi:

\subsubsection{Fase 1: Inizializzazione e Configurazione}

La fase di inizializzazione comprende:

\begin{lstlisting}[caption={Inizializzazione dual tracking system e caricamento video}]
# Inizializzazione due sistemi di tracciamento indipendenti
sistema_tracciamento = Tracciamento()              # Pipeline baseline
sistema_tracciamento_stabilized_ma = Tracciamento()  # Pipeline stabilized

cattura_video = cv2.VideoCapture('video/traffico1.mp4')
if not cattura_video.isOpened():
    print("Impossibile trovare il video!")
    exit()

stato_lettura, fotogramma_iniziale = cattura_video.read()
if not stato_lettura:
    print("Acquisizione fotogramma iniziale fallita!")
    exit()

altezza_fotogramma, larghezza_fotogramma = fotogramma_iniziale.shape[:2]

# Inizializzazione Video Stabilizer con frame di riferimento
stabilizer = VideoStabilizer()
stabilizer.set_reference_frame(fotogramma_iniziale)
\end{lstlisting}

Vengono istanziati \textbf{due oggetti \texttt{Tracciamento}} indipendenti: uno per la pipeline baseline (frame originali) e uno per la pipeline stabilized+filtered. Il video viene caricato mediante la classe \texttt{VideoCapture} di OpenCV. Il primo fotogramma viene utilizzato come \textbf{frame di riferimento} per la stabilizzazione video.

\subsubsection{Fase 2: Selezione della Region of Interest (ROI)}

Il sistema offre tre modalità per la definizione della ROI:

\begin{enumerate}
    \item \textbf{Modalità manuale}: l'utente seleziona interattivamente un'area rettangolare mediante interfaccia grafica
    \item \textbf{Modalità automatica}: il sistema definisce automaticamente una ROI centrata contenente il 60\% dell'area totale
    \item \textbf{Fotogramma completo}: l'intera area del fotogramma viene considerata come ROI
\end{enumerate}

\begin{lstlisting}[caption={Configurazione automatica della ROI}]
if scelta_modalita == "2":
    percentuale_larghezza_roi = 0.6
    percentuale_altezza_roi = 0.6
    
    larghezza_roi = int(larghezza_fotogramma * percentuale_larghezza_roi)
    altezza_roi = int(altezza_fotogramma * percentuale_altezza_roi)
    pos_x_roi = (larghezza_fotogramma - larghezza_roi) // 2
    pos_y_roi = (altezza_fotogramma - altezza_roi) // 2
\end{lstlisting}

La possibilità di limitare l'elaborazione a una ROI consente di ridurre il carico computazionale concentrandosi sulle aree di interesse, particolarmente utile in scenari dove il movimento si concentra in regioni specifiche del fotogramma.

\subsubsection{Fase 3: Inizializzazione Background Subtractor}

Il rilevatore di foreground viene inizializzato con l'algoritmo MOG2:

\begin{lstlisting}[caption={Inizializzazione MOG2 per entrambe le pipeline}]
# MOG2 per pipeline baseline (frame originali)
rilevatore_entita = cv2.createBackgroundSubtractorMOG2(
    history=200, 
    varThreshold=40
)

# MOG2 per pipeline stabilized+filtered
rilevatore_entita_stabilized_ma = cv2.createBackgroundSubtractorMOG2(
    history=200,
    varThreshold=40
)
\end{lstlisting}

I parametri configurati sono:
\begin{itemize}
    \item \texttt{history=200}: numero di fotogrammi considerati per la costruzione del modello di background
    \item \texttt{varThreshold=40}: soglia sulla distanza di Mahalanobis al quadrato per classificare un pixel come foreground
\end{itemize}

Vengono istanziati \textbf{due background subtractor indipendenti} per evitare contaminazione tra le due pipeline, permettendo confronto equo.

\subsubsection{Fase 3.5: Kernel Morfologico}

Si definisce il kernel per le operazioni morfologiche:

\begin{lstlisting}[caption={Definizione kernel ellittico}]
kernel_morph = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))
\end{lstlisting}

Un kernel ellittico $7 \times 7$ garantisce operazioni morfologiche isotrope (uniformi in tutte le direzioni), adatto per fusione di regioni di oggetti con forma irregolare.

\subsubsection{Fase 4: Loop di Elaborazione Fotogrammi}

Il ciclo principale elabora sequenzialmente tutti i fotogrammi del video implementando le due pipeline parallele:

\begin{lstlisting}[caption={Loop principale con dual-pipeline}]
in_pausa = False  # Flag per pausa video

while True:
    # Gestione controlli tastiera
    tasto = cv2.waitKey(1) & 0xFF
    if tasto == ord('q'):
        break
    elif tasto == ord(' '):  # SPAZIO per pausa/resume
        in_pausa = not in_pausa
    
    # Loop di attesa durante pausa
    while in_pausa:
        tasto = cv2.waitKey(100) & 0xFF
        if tasto == ord(' '):
            in_pausa = False
        elif tasto == ord('q'):
            break
    
    if tasto == ord('q'):
        break
        
    stato_acquisizione, fotogramma_attuale = cattura_video.read()
    if not stato_acquisizione:
        break

    # Estrazione ROI
    regione_interesse = fotogramma_attuale[
        pos_y_roi:pos_y_roi+altezza_roi, 
        pos_x_roi:pos_x_roi+larghezza_roi
    ]
    
    # ===== PIPELINE STABILIZED+FILTERED =====
    # 1. Stabilizzazione video
    frame_stabilizzato = stabilizer.stabilize(fotogramma_attuale.copy())
    
    # 2. Moving Average Spaziale (riduzione rumore)
    frame_ma = moving_average_filter(frame_stabilizzato)
    
    # Estrazione ROI dal frame processato
    roi_stabilized_ma = frame_ma[
        pos_y_roi:pos_y_roi+altezza_roi,
        pos_x_roi:pos_x_roi+larghezza_roi
    ]
    
    # 3. Background subtraction
    maschera_stabilized_ma = rilevatore_entita_stabilized_ma.apply(
        roi_stabilized_ma
    )
    _, maschera_stabilized_ma = cv2.threshold(
        maschera_stabilized_ma, 254, 255, cv2.THRESH_BINARY
    )
    
    # 4. Operazioni morfologiche
    # Closing: fusione regioni frammentate (tetto+cofano+finestrini)
    maschera_stabilized_ma = cv2.morphologyEx(
        maschera_stabilized_ma, cv2.MORPH_CLOSE, kernel_morph, 
        iterations=2
    )
    # Opening: rimozione rumore
    maschera_stabilized_ma = cv2.morphologyEx(
        maschera_stabilized_ma, cv2.MORPH_OPEN, kernel_morph, 
        iterations=1
    )
    
    # ===== PIPELINE BASELINE (frame originale) =====
    maschera_rilevamento = rilevatore_entita.apply(regione_interesse)
    _, maschera_rilevamento = cv2.threshold(
        maschera_rilevamento, 254, 255, cv2.THRESH_BINARY
    )
\end{lstlisting}

Il sistema implementa:
\begin{itemize}
    \item \textbf{Controlli tastiera}: Q per uscita, SPAZIO per pausa/resume
    \item \textbf{Stabilizzazione} del frame completo mediante ORB feature matching
    \item \textbf{Moving average spaziale} (kernel $5 \times 5$) per smoothing
    \item \textbf{Background subtraction} indipendente per ciascuna pipeline
    \item \textbf{Operazioni morfologiche} solo su pipeline stabilized: closing (2 iterazioni) + opening (1 iterazione)
\end{itemize}

\subsubsection{Fase 5: Rilevamento Contorni, NMS e Tracking Dual-Pipeline}

Sulla maschera binaria si individuano i contorni degli oggetti in foreground per entrambe le pipeline:

\begin{lstlisting}[caption={Rilevamento contorni, NMS e tracking parallelo}]
# ===== PIPELINE BASELINE =====
contorni_rilevati, _ = cv2.findContours(
    maschera_rilevamento, 
    cv2.RETR_EXTERNAL,  # Solo contorni esterni
    cv2.CHAIN_APPROX_SIMPLE
)

rilevamenti_validi = []
for singolo_contorno in contorni_rilevati:
    superficie_contorno = cv2.contourArea(singolo_contorno)
    if superficie_contorno > 500:  # Soglia area aumentata
        coord_x, coord_y, dimensione_w, dimensione_h = \
            cv2.boundingRect(singolo_contorno)
        rilevamenti_validi.append([coord_x, coord_y, 
                                   dimensione_w, dimensione_h])

# Tracking pipeline baseline
risultati_tracciamento = sistema_tracciamento.update(rilevamenti_validi)

# ===== PIPELINE STABILIZED+FILTERED =====
contorni_stabilized, _ = cv2.findContours(
    maschera_stabilized_ma,
    cv2.RETR_EXTERNAL,
    cv2.CHAIN_APPROX_SIMPLE
)

rilevamenti_stabilized_ma = []
for contorno in contorni_stabilized:
    area = cv2.contourArea(contorno)
    if area > 500:
        x, y, w, h = cv2.boundingRect(contorno)
        rilevamenti_stabilized_ma.append([x, y, w, h])

# 5. NMS: eliminazione detection duplicate
rilevamenti_stabilized_ma = filtra_box_sovrapposti(
    rilevamenti_stabilized_ma, 
    iou_threshold=0.3
)

# Tracking pipeline stabilized+filtered
risultati_stabilized_ma = sistema_tracciamento_stabilized_ma.update(
    rilevamenti_stabilized_ma
)
\end{lstlisting}

Le modifiche chiave rispetto alla versione baseline:
\begin{enumerate}
    \item \textbf{RETR\_EXTERNAL}: estrae solo contorni esterni, evitando contorni gerarchici interni
    \item \textbf{NMS con IoU threshold 0.3}: elimina bounding box sovrapposte mantenendo quella con area maggiore
    \item \textbf{Tracking parallelo}: due sistemi di tracciamento indipendenti per confronto
\end{enumerate}

La funzione \texttt{filtra\_box\_sovrapposti} implementa l'algoritmo NMS:

\begin{lstlisting}[caption={Algoritmo NMS con IoU}]
def filtra_box_sovrapposti(boxes, iou_threshold=0.5):
    if len(boxes) == 0:
        return []
    
    # Ordinamento per area decrescente
    boxes_sorted = sorted(boxes, 
                         key=lambda b: b[2] * b[3], 
                         reverse=True)
    
    filtered_boxes = []
    while boxes_sorted:
        # Selezione box con area massima
        current_box = boxes_sorted.pop(0)
        filtered_boxes.append(current_box)
        
        # Rimozione box sovrapposti
        boxes_sorted = [
            box for box in boxes_sorted 
            if calcola_iou(current_box, box) < iou_threshold
        ]
    
    return filtered_boxes
\end{lstlisting}

\subsubsection{Fase 7: Applicazione Algoritmi e Memorizzazione}

Per ciascun fotogramma si applicano tre algoritmi di elaborazione distinti, memorizzando le sequenze risultanti:

\begin{lstlisting}[caption={Applicazione algoritmi di qualità}]
# Compressione JPEG
fotogramma_solo_jpeg = jpeg_compression(
    fotogramma_attuale.copy(), quality=50
)
fotogrammi_jpeg.append(fotogramma_solo_jpeg)

# Sfocatura Gaussiana
fotogramma_solo_blurred = blurring(
    fotogramma_attuale.copy(), kernel_size=(5, 5), sigma=0
)
fotogrammi_blurred.append(fotogramma_solo_blurred)

# Unsharp Masking (Sharpening)
fotogramma_solo_sharpened = unsharp_masking(
    fotogramma_attuale.copy(), amount=1.5
)
fotogrammi_sharpened.append(fotogramma_solo_sharpened)
\end{lstlisting}

È fondamentale passare copie dei fotogrammi alle funzioni di elaborazione per preservare l'originale intatto durante il confronto metrico.

\subsubsection{Fase 8: Visualizzazione Real-Time Dual-Pipeline}

Il sistema presenta contemporaneamente finestre multiple per il monitoraggio comparativo real-time:

\textbf{Finestre Pipeline Baseline:}
\begin{itemize}
    \item ROI originale con annotazioni tracking (ID e bounding box verdi)
    \item Maschera background subtraction baseline
\end{itemize}

\textbf{Finestre Pipeline Stabilized+Filtered:}
\begin{itemize}
    \item Frame stabilizzato completo (output VideoStabilizer)
    \item Frame stabilizzato + moving average spatial
    \item ROI stabilized+MA con annotazioni tracking (ID e bounding box blu per distinzione)
    \item Maschera background subtraction dopo operazioni morfologiche
\end{itemize}

\textbf{Finestre Valutazione Qualità (su frame originale):}
\begin{itemize}
    \item Sequenza compressa JPEG (Q=50)
    \item Sequenza blurred (Gaussian 5x5)
    \item Sequenza sharpened (Unsharp Masking 1.5x)
\end{itemize}

\begin{lstlisting}[caption={Visualizzazione finestre dual-pipeline}]
# Pipeline Baseline
cv2.imshow("ROI Original + Tracking", regione_interesse)
cv2.imshow("Mask Original", maschera_ridimensionata)

# Pipeline Stabilized+Filtered
cv2.imshow("Frame Stabilized", frame_stabilizzato)
cv2.imshow("Frame Stabilized + MA", frame_ma)
cv2.imshow("ROI Stabilized+MA + Tracking", roi_stabilized_ma_display)
cv2.imshow("Mask Stabilized+MA (after morphology)", 
           maschera_stabilized_ridim)

# Valutazione Qualità
cv2.imshow("JPEG Compressed (Q=50)", fotogramma_jpeg)
cv2.imshow("Gaussian Blur (5x5)", fotogramma_blurred)
cv2.imshow("Unsharp Masked (1.5x)", fotogramma_sharpened)

# Controlli: Q=quit, SPAZIO=pause/resume
tasto = cv2.waitKey(1) & 0xFF
if tasto == ord('q'):
    break
elif tasto == ord(' '):
    in_pausa = not in_pausa
\end{lstlisting}

La visualizzazione dual-pipeline consente confronto visivo immediato tra baseline e stabilized, evidenziando:
\begin{itemize}
    \item Riduzione falsi positivi nella maschera stabilizzata
    \item Fusione regioni frammentate (1 box invece di 3-5)
    \item Maggiore stabilità ID tracking (colori bounding box cambiano meno frequentemente)
\end{itemize}

\subsubsection{Fase 9: Calcolo Metriche di Qualità}

Terminato il loop di elaborazione, si calcolano le metriche confrontando le sequenze:

\begin{lstlisting}[caption={Calcolo metriche comparative}]
# ORIGINALE vs JPEG Compression
mse_jpeg, psnr_jpeg, ssim_jpeg = compare_video_sequences(
    sequenza_fotogrammi, fotogrammi_jpeg
)

# ORIGINALE vs Gaussian Blurring
mse_blurred, psnr_blurred, ssim_blurred = compare_video_sequences(
    sequenza_fotogrammi, fotogrammi_blurred
)

# ORIGINALE vs Unsharp Masking
mse_sharpened, psnr_sharpened, ssim_sharpened = compare_video_sequences(
    sequenza_fotogrammi, fotogrammi_sharpened
)
\end{lstlisting}

I risultati vengono stampati a console fornendo una valutazione quantitativa dell'impatto di ciascun algoritmo sulla qualità dell'immagine.

% ============================================================================
% CAPITOLO 4: ALGORITMO DI TRACKING
% ============================================================================

\section{Algoritmo di Tracking}

\subsection{Classe Tracciamento}

Il modulo \texttt{tracciamento.py} implementa la classe \texttt{Tracciamento}, responsabile dell'associazione temporale degli oggetti rilevati attraverso i fotogrammi successivi mediante \textbf{ORB feature matching} combinato con distanza euclidea e coordinate smoothing. L'obiettivo è mantenere identificatori univoci e persistenti per ciascun oggetto durante tutto il periodo in cui rimane visibile nella scena.

\subsubsection{Struttura della Classe}

La classe mantiene strutture dati per feature matching e smoothing:

\begin{lstlisting}[caption={Inizializzazione classe Tracciamento avanzata}]
import cv2
import math
import numpy as np

class Tracciamento:
    def __init__(self, distanza_max=350):
        # Contatore incrementale per ID univoci
        self.identificativo_corrente = 0
        
        # Dizionario: ID -> (centro_x, centro_y)
        self.coordinate_centrali = {}
        
        # ORB detector per feature extraction (100 features)
        self.orb = cv2.ORB_create(nfeatures=100)
        
        # BruteForce Matcher con distanza Hamming
        self.bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)
        
        # Dizionario: ID -> (keypoints, descriptors)
        self.features_oggetti = {}
        
        # Coordinate smoothing per riduzione jitter
        self.coordinate_smoothing = CoordinateSmoothing(alpha=0.3)
        
        # Soglia distanza massima configurabile
        self.distanza_max = distanza_max
\end{lstlisting}

\subsection{Metodo di Aggiornamento con Feature Matching}

Il metodo \texttt{update()} integra feature matching ORB con distanza euclidea per associazione robusta:

\begin{lstlisting}[caption={Metodo update con ORB feature matching}]
def update(self, rettangoli_rilevati, frame_roi):
    risultati_tracciamento = []
    
    for rettangolo in rettangoli_rilevati:
        x, y, w, h = rettangolo
        
        # Calcolo centroide
        centro_x = (x + x + w) // 2
        centro_y = (y + y + h) // 2
        
        # Estrazione ROI oggetto per feature extraction
        roi_oggetto = frame_roi[y:y+h, x:x+w]
        
        # Conversione grayscale e feature extraction
        if len(roi_oggetto.shape) == 3:
            roi_gray = cv2.cvtColor(roi_oggetto, cv2.COLOR_BGR2GRAY)
        else:
            roi_gray = roi_oggetto
            
        kp_nuovo, desc_nuovo = self.orb.detectAndCompute(
            roi_gray, None
        )
        
        # Ricerca migliore match tra oggetti tracciati
        best_match_id = None
        best_score = 0
        
        for id_obj, (kp_old, desc_old) in self.features_oggetti.items():
            if desc_nuovo is None or desc_old is None:
                continue
            
            # Feature matching
            matches = self.bf.knnMatch(desc_nuovo, desc_old, k=2)
            
            # Ratio test di Lowe
            good_matches = []
            for match_pair in matches:
                if len(match_pair) == 2:
                    m, n = match_pair
                    if m.distance < 0.75 * n.distance:
                        good_matches.append(m)
            
            # Calcolo score combinato (70% features, 30% distanza)
            feature_score = len(good_matches) / max(len(kp_nuovo), 
                                                    len(kp_old))
            
            centro_old = self.coordinate_centrali[id_obj]
            dist_eucl = math.hypot(centro_x - centro_old[0],
                                  centro_y - centro_old[1])
            
            if dist_eucl > self.distanza_max:
                continue
            
            dist_score = 1.0 - (dist_eucl / self.distanza_max)
            
            combined_score = 0.7 * feature_score + 0.3 * dist_score
            
            if combined_score > best_score:
                best_score = combined_score
                best_match_id = id_obj
        
        # Soglia minima score per accettazione match
        if best_match_id is not None and best_score > 0.3:
            # Aggiornamento oggetto esistente
            # Coordinate smoothing per riduzione jitter
            centro_smooth = self.coordinate_smoothing.smooth(
                best_match_id, (centro_x, centro_y)
            )
            
            self.coordinate_centrali[best_match_id] = centro_smooth
            self.features_oggetti[best_match_id] = (kp_nuovo, 
                                                    desc_nuovo)
            
            risultati_tracciamento.append([
                best_match_id, x, y, w, h
            ])
        else:
            # Nuova traccia
            self.coordinate_centrali[self.identificativo_corrente] = \
                (centro_x, centro_y)
            self.features_oggetti[self.identificativo_corrente] = \
                (kp_nuovo, desc_nuovo)
            
            risultati_tracciamento.append([
                self.identificativo_corrente, x, y, w, h
            ])
            self.identificativo_corrente += 1
    
    # Pulizia tracce obsolete
    self._rimuovi_entita_obsolete(risultati_tracciamento)
    
    return risultati_tracciamento
\end{lstlisting}

\subsection{Coordinate Smoothing}

La classe \texttt{CoordinateSmoothing} implementa exponential moving average:

\begin{lstlisting}[caption={Classe CoordinateSmoothing}]
class CoordinateSmoothing:
    def __init__(self, alpha=0.3):
        self.alpha = alpha  # Fattore smoothing
        self.previous_positions = {}
    
    def smooth(self, obj_id, new_position):
        if obj_id not in self.previous_positions:
            self.previous_positions[obj_id] = new_position
            return new_position
        
        prev_x, prev_y = self.previous_positions[obj_id]
        new_x, new_y = new_position
        
        # Exponential moving average
        smooth_x = int(self.alpha * new_x + 
                      (1 - self.alpha) * prev_x)
        smooth_y = int(self.alpha * new_y + 
                      (1 - self.alpha) * prev_y)
        
        smoothed = (smooth_x, smooth_y)
        self.previous_positions[obj_id] = smoothed
        
        return smoothed
\end{lstlisting}

\subsection{Gestione Tracce Obsolete}

Il metodo privato \texttt{\_rimuovi\_entita\_obsolete()} elimina dal dizionario le tracce di oggetti non più rilevati:

\begin{lstlisting}[caption={Rimozione tracce obsolete}]
def _rimuovi_entita_obsolete(self, risultati_tracciamento):
    # Estrazione ID attivi
    id_attivi = {r[0] for r in risultati_tracciamento}
    
    # Identificazione ID da rimuovere
    id_da_rimuovere = [
        id_ent for id_ent in self.coordinate_centrali.keys() 
        if id_ent not in id_attivi
    ]
    
    # Rimozione
    for id_ent in id_da_rimuovere:
        del self.coordinate_centrali[id_ent]
        if id_ent in self.features_oggetti:
            del self.features_oggetti[id_ent]
\end{lstlisting}

Questo meccanismo garantisce che oggetti usciti dalla scena o occultati non mantengano indefinitamente risorse.

\subsection{Non-Maximum Suppression (NMS)}

La funzione \texttt{filtra\_box\_sovrapposti()} implementa l'algoritmo NMS con calcolo IoU:

\begin{lstlisting}[caption={Implementazione NMS completa}]
def filtra_box_sovrapposti(boxes, iou_threshold=0.5):
    """
    Elimina bounding box duplicate mediante NMS.
    
    Args:
        boxes: Lista di [x, y, w, h]
        iou_threshold: Soglia IoU per eliminazione (default 0.5)
    
    Returns:
        Lista filtrata di bounding box
    """
    if len(boxes) == 0:
        return []
    
    # Ordinamento per area decrescente
    boxes_sorted = sorted(boxes, 
                         key=lambda b: b[2] * b[3], 
                         reverse=True)
    
    filtered_boxes = []
    
    while boxes_sorted:
        # Selezione box con area massima
        current_box = boxes_sorted.pop(0)
        filtered_boxes.append(current_box)
        
        # Filtraggio box sovrapposti
        boxes_sorted = [
            box for box in boxes_sorted 
            if calcola_iou(current_box, box) < iou_threshold
        ]
    
    return filtered_boxes

def calcola_iou(box1, box2):
    """Calcola Intersection over Union tra due box."""
    x1, y1, w1, h1 = box1
    x2, y2, w2, h2 = box2
    
    # Coordinate intersezione
    xi1 = max(x1, x2)
    yi1 = max(y1, y2)
    xi2 = min(x1 + w1, x2 + w2)
    yi2 = min(y1 + h1, y2 + h2)
    
    # Area intersezione
    inter_width = max(0, xi2 - xi1)
    inter_height = max(0, yi2 - yi1)
    inter_area = inter_width * inter_height
    
    # Area unione
    box1_area = w1 * h1
    box2_area = w2 * h2
    union_area = box1_area + box2_area - inter_area
    
    # IoU
    if union_area == 0:
        return 0.0
    
    return inter_area / union_area
\end{lstlisting}

L'algoritmo garantisce eliminazione efficiente di detection duplicate preservando quella con area maggiore (tipicamente la più accurata).

\subsection{Limitazioni e Miglioramenti}


\subsubsection{Limitazioni}


\begin{enumerate}
    \item \textbf{Occlusioni complete prolungate}: se un oggetto viene completamente occultato per più di 2-3 frame, perde comunque il proprio ID
    
    \item \textbf{Assenza di predizione}: non si prevede la posizione futura dell'oggetto basandosi sulla traiettoria passata (approcci Kalman filter potrebbero migliorare questo aspetto)
    
\end{enumerate}


% ============================================================================
% CAPITOLO 5: POST-PROCESSING
% ============================================================================

\section{Algoritmi di Valutazione della Qualità}

Il modulo \texttt{filtri.py} implementa la classe \texttt{VideoStabilizer} per stabilizzazione video feature-based e quattro algoritmi di elaborazione video finalizzati alla valutazione della qualità dell'immagine mediante metriche quantitative.

\subsection{Video Stabilizer}

La classe \texttt{VideoStabilizer} implementa stabilizzazione feature-based utilizzando ORB detector e trasformazioni affini.

\subsubsection{Architettura della Classe}

\begin{lstlisting}[caption={Classe VideoStabilizer}]
import cv2
import numpy as np
from collections import deque

class VideoStabilizer:
    def __init__(self, smoothing_window=5):
        # ORB detector con 2000 features
        self.orb = cv2.ORB_create(nfeatures=2000)
        
        # BFMatcher per feature matching
        self.bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
        
        # Frame di riferimento
        self.reference_frame = None
        self.reference_kp = None
        self.reference_desc = None
        
        # Buffer per smoothing trasformazioni
        self.transform_buffer = deque(maxlen=smoothing_window)
        
    def set_reference_frame(self, frame):
        """Imposta frame di riferimento per stabilizzazione."""
        self.reference_frame = frame.copy()
        
        # Estrazione feature dal frame di riferimento
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        self.reference_kp, self.reference_desc = \
            self.orb.detectAndCompute(gray, None)
    
    def stabilize(self, frame):
        """
        Stabilizza un frame allineandolo al riferimento.
        
        Args:
            frame: Frame BGR da stabilizzare
        
        Returns:
            Frame stabilizzato (warped)
        """
        if self.reference_frame is None:
            return frame
        
        # Conversione grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Feature detection e matching
        kp, desc = self.orb.detectAndCompute(gray, None)
        
        if desc is None or self.reference_desc is None:
            return frame
        
        # Matching features
        matches = self.bf.match(self.reference_desc, desc)
        
        # Selezione best matches (almeno 10 richiesti)
        if len(matches) < 10:
            return frame
        
        # Ordinamento per distanza
        matches = sorted(matches, key=lambda x: x.distance)
        
        # Estrazione coordinate matched points
        src_pts = np.float32([
            self.reference_kp[m.queryIdx].pt for m in matches
        ]).reshape(-1, 1, 2)
        
        dst_pts = np.float32([
            kp[m.trainIdx].pt for m in matches
        ]).reshape(-1, 1, 2)
        
        # Stima trasformazione affine con RANSAC
        transform_matrix, inliers = cv2.estimateAffinePartial2D(
            dst_pts, src_pts, 
            method=cv2.RANSAC,
            ransacReprojThreshold=5.0
        )
        
        if transform_matrix is None:
            return frame
        
        # Smoothing trasformazione
        self.transform_buffer.append(transform_matrix)
        smooth_transform = np.mean(self.transform_buffer, axis=0)
        
        # Warping frame
        h, w = frame.shape[:2]
        stabilized = cv2.warpAffine(
            frame, smooth_transform, (w, h),
            flags=cv2.INTER_LINEAR,
            borderMode=cv2.BORDER_REFLECT
        )
        
        return stabilized
\end{lstlisting}

Il processo di stabilizzazione:
\begin{enumerate}
    \item Estrazione di 2000 feature ORB dal frame corrente
    \item Matching con feature del frame di riferimento
    \item Stima trasformazione affine ottimale (RANSAC per robustezza)
    \item Smoothing della trasformazione (media mobile su 5 frame)
    \item Applicazione warping per allineare frame al riferimento
\end{enumerate}

\subsection{Moving Average Spatial Filter}

Il filtro moving average spaziale applica smoothing uniforme mediante convoluzione:

\begin{lstlisting}[caption={Moving average spaziale}]
def moving_average_filter(frame, kernel_size=5):
    """
    Applica moving average spaziale (smoothing uniforme).
    
    Args:
        frame: Immagine BGR
        kernel_size: Dimensione kernel (default 5x5)
    
    Returns:
        Immagine filtrata
    """
    kernel = np.ones((kernel_size, kernel_size), np.float32) / \
             (kernel_size * kernel_size)
    
    filtered = cv2.filter2D(frame, -1, kernel)
    
    return filtered
\end{lstlisting}

Il kernel uniforme $5 \times 5$ produce smoothing moderato riducendo rumore ad alta frequenza senza degradare eccessivamente i dettagli.

\subsection{Compressione JPEG}

La compressione JPEG è uno standard lossy ampiamente utilizzato che introduce artefatti di quantizzazione caratteristici. L'algoritmo opera trasformando l'immagine dallo spazio RGB a YCbCr, applicando la Discrete Cosine Transform (DCT) su blocchi $8 \times 8$ pixel, e quantizzando i coefficienti ad alta frequenza.

Il livello di compressione è controllato dal parametro \texttt{quality} $\in [0, 100]$, dove valori inferiori producono file più piccoli ma maggiori distorsioni visive. Gli artefatti tipici includono:

\begin{itemize}
    \item \textbf{Blocking artifacts}: discontinuità tra blocchi adiacenti
    \item \textbf{Ringing}: oscillazioni spurie vicino ai bordi netti
    \item \textbf{Color bleeding}: perdita di dettaglio cromatico dovuta al subsampling dei canali di crominanza
\end{itemize}

\subsubsection{Implementazione}

\begin{lstlisting}[caption={Funzione jpeg\_compression}]
def jpeg_compression(immagine_input, quality=50):
    """
    Applica compressione JPEG con livello di qualita specificato.
    
    Args:
        immagine_input: Immagine BGR di input
        quality: Livello qualita JPEG (0-100, default=50)
    
    Returns:
        Immagine BGR compressa e decompressa
    """
    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]
    result, encoded_img = cv2.imencode('.jpg', immagine_input, 
                                       encode_param)
    decoded_img = cv2.imdecode(encoded_img, cv2.IMREAD_COLOR)
    return decoded_img
\end{lstlisting}

Il valore \texttt{quality=50} rappresenta un compromesso intermedio tra dimensione file e fedeltà visiva. Questa trasformazione introduce degradazioni misurabilidalle metriche: aumento di MSE, diminuzione di PSNR e SSIM.

\subsection{Sfocatura Gaussiana}

La sfocatura gaussiana rappresenta un filtro di smoothing lineare che riduce il rumore ad alta frequenza e i dettagli fini tramite convoluzione con un kernel gaussiano bidimensionale:

\begin{equation}
G(x, y) = \frac{1}{2\pi\sigma^2} e^{-\frac{x^2 + y^2}{2\sigma^2}}
\end{equation}

dove $\sigma$ è la deviazione standard che controlla l'ampiezza della distribuzione. Valori maggiori producono sfocature più pronunciate. Il filtro gaussiano è separabile, permettendo un'implementazione efficiente mediante due convoluzioni 1D sequenziali.

\subsubsection{Implementazione}

\begin{lstlisting}[caption={Funzione blurring}]
def blurring(immagine_input, kernel_size=(5, 5), sigma=0):
    """
    Applica sfocatura gaussiana all'immagine.
    
    Args:
        immagine_input: Immagine BGR di input
        kernel_size: Dimensioni kernel (dispari, default=(5,5))
        sigma: Deviazione standard (0=auto, default=0)
    
    Returns:
        Immagine BGR sfocata
    """
    immagine_sfocata = cv2.GaussianBlur(immagine_input, 
                                        kernel_size, sigma)
    return immagine_sfocata
\end{lstlisting}

Con \texttt{kernel\_size=(5, 5)} e \texttt{sigma=0} (calcolo automatico), il filtro produce un moderato effetto di smoothing. L'applicazione riduce MSE dovuto al rumore ma degrada SSIM per la perdita di dettagli ad alta frequenza.

\subsection{Unsharp Masking}

L'unsharp masking è una tecnica di sharpening che aumenta il contrasto locale in prossimità dei bordi. Il processo si articola in tre fasi:

\begin{enumerate}
    \item Creazione di una versione sfocata dell'immagine originale
    \item Sottrazione della versione sfocata dall'originale per ottenere una "maschera" contenente i dettagli ad alta frequenza
    \item Somma della maschera amplificata all'immagine originale
\end{enumerate}

Matematicamente:

\begin{equation}
I_{\text{sharp}} = I + \alpha \cdot (I - I_{\text{blur}})
\end{equation}

dove $\alpha$ è il parametro \texttt{amount} che controlla l'intensità dello sharpening.

\subsubsection{Implementazione}

\begin{lstlisting}[caption={Funzione unsharp\_masking}]
def unsharp_masking(immagine_input, kernel_size=(5, 5), 
                    sigma=1.0, amount=1.5, threshold=0):
    """
    Applica unsharp masking per aumentare nitidezza.
    
    Args:
        immagine_input: Immagine BGR di input
        kernel_size: Dimensioni kernel Gaussian (default=(5,5))
        sigma: Deviazione standard blur (default=1.0)
        amount: Intensita sharpening (default=1.5)
        threshold: Soglia minima contrasto (default=0)
    
    Returns:
        Immagine BGR sharpened
    """
    # Creazione versione sfocata
    blurred = cv2.GaussianBlur(immagine_input, kernel_size, 
                               sigma)
    
    # Calcolo maschera dettagli
    mask = cv2.subtract(immagine_input, blurred)
    
    # Applicazione maschera amplificata
    sharpened = cv2.addWeighted(immagine_input, 1.0, 
                                 mask, amount, 0)
    
    # Clipping valori
    sharpened = np.clip(sharpened, 0, 255).astype(np.uint8)
    
    return sharpened
\end{lstlisting}

Con \texttt{amount=1.5}, lo sharpening enfatizza moderatamente i bordi senza introdurre artefatti di halo eccessivi. Questa trasformazione può migliorare la nitidezza percettiva mantenendo buoni valori SSIM.

\subsection{Conclusioni}

Gli algoritmi implementati rappresentano categorie distinte di elaborazione con impatti differenziati sulla qualità:

\begin{itemize}
    \item \textbf{Video Stabilization}: compensazione movimenti camera preservando contenuto (MSE minimo, SSIM alto se camera stabile)
    
    \item \textbf{Moving Average Spatial}: smoothing uniforme riducendo rumore ad alta frequenza (MSE variabile, lieve degradazione SSIM)
    
    \item \textbf{JPEG compression}: introduce degradazioni lossy quantificabili (MSE $\uparrow$, PSNR $\downarrow$, SSIM $\downarrow$)
    
    \item \textbf{Gaussian blurring}: filtraggio passa-basso con perdita dettagli (MSE variabile, SSIM $\downarrow$)
    
    \item \textbf{Unsharp masking}: enfatizza bordi e dettagli (SSIM stabile/migliorata se moderato)
\end{itemize}

Le trasformazioni di stabilizzazione e moving average sono \textbf{preprocessing} applicati prima del tracking per migliorarne robustezza, mentre JPEG/blur/sharpening sono \textbf{post-processing} per valutazione qualità mediante metriche. Queste ultime consentono di testare la sensibilità e la capacità discriminativa delle metriche MSE, PSNR e SSIM rispetto a diversi tipi di distorsioni e miglioramenti, con SSIM che meglio riflette la percezione visiva umana.

% ============================================================================
% CAPITOLO 6: METRICHE DI QUALITÀ
% ============================================================================

\section{Calcolo Metriche di Qualità}

Il modulo \texttt{Calcolo\_metriche.py} implementa la funzione \texttt{compare\_video\_sequences()} che calcola MSE, PSNR e SSIM confrontando fotogramma per fotogramma due sequenze video.

\subsection{Architettura della Funzione}

Questa implementazione confronta fotogrammi corrispondenti di due sequenze distinte: l'originale e quella trasformata. Questo approccio full-reference quantifica l'impatto della trasformazione applicata.

\subsubsection{Validazione Input}

\begin{lstlisting}[caption={Validazione sequenze video}]
def compare_video_sequences(sequenza_originale, sequenza_filtrata):
    # Verifica lunghezze
    if len(sequenza_originale) != len(sequenza_filtrata):
        print(f"ERRORE: Le due sequenze hanno lunghezze diverse!")
        print(f"  Originale: {len(sequenza_originale)} frame")
        print(f"  Filtrata:  {len(sequenza_filtrata)} frame")
        return None, None, None
    
    # Verifica non vuote
    if len(sequenza_originale) < 1:
        print("ERRORE: Sequenze vuote!")
        return None, None, None
    
    numero_fotogrammi = len(sequenza_originale)
\end{lstlisting}

La funzione verifica che le due sequenze abbiano la stessa lunghezza (requisito fondamentale per confronti frame-by-frame) e che contengano almeno un fotogramma.

\subsection{Calcolo MSE}

L'errore quadratico medio viene calcolato come:

\begin{lstlisting}[caption={Calcolo MSE per singolo frame}]
errore_quadratico = np.mean(
    (frame_orig.astype(float) - frame_filt.astype(float)) ** 2
)
somma_mse += errore_quadratico
\end{lstlisting}

L'MSE finale è la media su tutti i fotogrammi:

\begin{lstlisting}[caption={MSE medio su sequenza}]
mse_medio = somma_mse / numero_fotogrammi
\end{lstlisting}

\subsection{Calcolo PSNR}

Il PSNR viene calcolato in decibel applicando la formula logaritmica:

\begin{lstlisting}[caption={Calcolo PSNR per singolo frame}]
if errore_quadratico == 0:
    # Frame identici: PSNR molto alto
    rapporto_segnale = 100.0
else:
    valore_massimo_pixel = 255.0
    rapporto_segnale = 20 * np.log10(
        valore_massimo_pixel / np.sqrt(errore_quadratico)
    )

lista_psnr.append(rapporto_segnale)
somma_psnr += rapporto_segnale
\end{lstlisting}

Il PSNR medio è:

\begin{lstlisting}[caption={PSNR medio su sequenza}]
psnr_medio = somma_psnr / numero_fotogrammi
\end{lstlisting}

\subsection{Calcolo SSIM}

L'SSIM richiede immagini in scala di grigi. Si effettua la conversione se necessario:

\begin{lstlisting}[caption={Calcolo SSIM per singolo frame}]
# Conversione a grayscale per SSIM
if len(frame_orig.shape) == 3:
    frame_orig_gray = cv2.cvtColor(frame_orig, cv2.COLOR_BGR2GRAY)
    frame_filt_gray = cv2.cvtColor(frame_filt, cv2.COLOR_BGR2GRAY) \
        if len(frame_filt.shape) == 3 else frame_filt
else:
    frame_orig_gray = frame_orig
    frame_filt_gray = frame_filt

# Calcolo SSIM mediante libreria scikit-image
punteggio_similarita, _ = ssim(
    frame_orig_gray, frame_filt_gray, full=True
)
somma_ssim += punteggio_similarita
\end{lstlisting}

La funzione \texttt{ssim()} della libreria \texttt{scikit-image} implementa l'algoritmo SSIM. Il parametro \texttt{full=True} restituisce anche la mappa completa di similarità locale (non utilizzata in questa implementazione). L'SSIM medio è:

\begin{lstlisting}[caption={SSIM medio su sequenza}]
ssim_medio = somma_ssim / numero_fotogrammi
\end{lstlisting}

\subsection{Restituzione Risultati}

La funzione termina restituendo la tripla di metriche:

\begin{lstlisting}[caption={Restituzione metriche}]
return mse_medio, psnr_medio, ssim_medio
\end{lstlisting}

Questi valori quantificano rispettivamente:
\begin{itemize}
    \item \textbf{MSE}: errore quadratico medio (più basso = migliore similarità)
    \item \textbf{PSNR}: rapporto segnale-rumore in dB (più alto = migliore qualità)
    \item \textbf{SSIM}: similarità strutturale (più vicino a 1 = migliore similarità percettiva)
\end{itemize}

\section{Risultati Sperimentali}

\subsection{Setup Sperimentale}

Gli esperimenti sono stati condotti su video di prova (\texttt{traffico1.mp4}, \texttt{traffico2.mp4}) contenenti veicoli in movimento con presenza di camera shake solo nel secondo video.


\subsubsection{Configurazione Parametri}

\begin{itemize}
    \item \textbf{MOG2}: history=200, varThreshold=40
    \item \textbf{Area minima detection}: 500 pixel (filtro rumore)
    \item \textbf{Kernel morfologico}: ellisse $7 \times 7$, closing (2 iter) + opening (1 iter)
    \item \textbf{NMS IoU threshold}: 0.3
    \item \textbf{ORB features stabilizzazione}: 2000
    \item \textbf{ORB features tracking}: 100
    \item \textbf{Distanza max tracking}: 75 pixel (configurabile dall'utente)
    \item \textbf{Coordinate smoothing $\alpha$}: 0.3
\end{itemize}

\subsection{Tracciamento Oggetti: Confronto Dual-Pipeline}

Il sistema dual-pipeline consente confronto visivo diretto tra approccio baseline e approccio stabilized+filtered. I test sono stati condotti sul video \texttt{traffico1.mp4} (risoluzione 1280x720, telecamera fissa), utilizzando l'intero fotogramma come ROI e configurando la distanza euclidea massima a 75 pixel.

\subsubsection{Risultati Quantitativi su traffico1.mp4}

Il confronto tra le due pipeline ha prodotto i seguenti risultati:

\begin{table}[H]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Pipeline} & \textbf{Oggetti Rilevati} \\
\hline
Baseline (Frame Originali) & 204 \\
Stabilized + Moving Average Spaziale & 33 \\
\hline
\textbf{Differenza (Falsi Positivi)} & \textbf{+171} \\
\hline
\end{tabular}
\caption{Confronto quantitativo detection tra le due pipeline}
\label{tab:dual_pipeline_results}
\end{table}

\textbf{Interpretazione:} Il numero di 33 oggetti rilevati nella pipeline stabilized+filtered è \textbf{molto vicino al numero effettivo di automobili presenti nel video}, mentre i 204 oggetti della pipeline baseline includono 171 falsi positivi. 

\subsubsection{Analisi Qualitativa: Riduzione Falsi Positivi}

\textbf{Contesto tecnico:} Nonostante il video \texttt{traffico1.mp4} sia stato acquisito con \textbf{telecamera fissa} (assenza di camera shake intenzionale), la pipeline baseline genera numerosi falsi positivi derivanti da:
\begin{itemize}
    \item Micromovimenti naturali: oscillazione erba, foglie degli alberi, riflessi
    \item Jitter digitale: piccole vibrazioni del sensore camera
    \item Variazioni illuminazione: ombre, nuvole
    \item Rumore elettronico: instabilità acquisizione video
\end{itemize}

La Figura \ref{fig:comparison_detection} mostra il confronto visivo tra le detection delle due pipeline sullo stesso frame:

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{immagini/auto1originale.png}
    \caption{Pipeline Baseline: 204 oggetti totali. Visibili numerosi falsi positivi su erba e alberi in movimento.}
    \label{fig:baseline_detection}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{immagini/auto1stabilized+blur+features.png}
    \caption{Pipeline Stabilized+MA: 33 oggetti totali (solo automobili). Micromovimenti erba/alberi completamente filtrati.}
    \label{fig:stabilized_detection}
\end{subfigure}
\caption{Confronto detection dual-pipeline: evidenza riduzione falsi positivi}
\label{fig:comparison_detection}
\end{figure}

\textbf{Osservazioni chiave dalla Figura \ref{fig:comparison_detection}:}
\begin{enumerate}
    \item \textbf{Baseline (sinistra)}: l'oscillazione naturale dell'erba e il movimento delle foglie degli alberi vengono erroneamente interpretati come oggetti in movimento dal MOG2, generando bounding box spurie nelle aree vegetali.
    
    \item \textbf{Stabilized+MA (destra)}: la combinazione di stabilizzazione video e filtro moving average spaziale \textbf{elimina completamente} le detection su erba e alberi, rilevando esclusivamente i veicoli effettivamente in movimento sulla strada.
    
    \item La differenza di 171 oggetti corrisponde quasi interamente a questi micromovimenti ambientali erroneamente classificati come foreground.
\end{enumerate}

\subsubsection{Analisi Maschere MOG2: Riduzione Rumore}

La Figura \ref{fig:mask_comparison} confronta le maschere binarie generate dal background subtractor MOG2 nelle due pipeline:

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{immagini/rumorejitterdifferenze.png}
\caption{Confronto maschere MOG2: originale (sinistra) vs stabilized+MA (destra). Evidente riduzione rumore e miglioramento qualità segmentazione.}
\label{fig:mask_comparison}
\end{figure}

\textbf{Analisi comparativa delle maschere (Figura \ref{fig:mask_comparison}):}

\begin{itemize}
    \item \textbf{Maschera Originale (sinistra)}: presenta elevato rumore sale-pepe, contorni frastagliati, frammentazione delle regioni foreground, e numerose regioni spurie disperse nel fotogramma. Il jitter digitale e i micromovimenti causano instabilità pixel-level che si propagano come noise nella maschera binaria.
    
    \item \textbf{Maschera Stabilized+MA (destra)}: mostra regioni foreground compatte e pulite, contorni ben definiti, assenza quasi totale di rumore isolato, e continuità spaziale delle detection. La stabilizzazione elimina il jitter globale, mentre il filtro moving average spaziale riduce il rumore ad alta frequenza.
    
    \item \textbf{Impatto sul tracking}: la qualità superiore della maschera filtrata si traduce in:
    \begin{itemize}
        \item Bounding box più stabili e accurate
        \item Riduzione drastica dei falsi positivi
        \item Contorni più precisi per estrazione ROI feature matching ORB
        \item Minore frammentazione oggetti (operazioni morfologiche più efficaci su maschere pulite)
    \end{itemize}
\end{itemize}

\subsubsection{Conclusioni Comparative}

I risultati dimostrano che, anche in presenza di \textbf{telecamera fissa} (senza shake intenzionale), la pipeline stabilized+filtered fornisce benefici sostanziali:

\begin{enumerate}
    \item \textbf{Riduzione falsi positivi}: da 204 a 33 detection ($-83.8\%$), eliminando praticamente tutti i micromovimenti ambientali
    
    \item \textbf{Accuracy migliorata}: 33 oggetti rilevati $\approx$ numero reale automobili nel video
    
    \item \textbf{Qualità maschere}: riduzione rumore, contorni più definiti, regioni compatte
    
    \item \textbf{Robustezza tracking}: ID più stabili, bounding box accurate, minore frammentazione
\end{enumerate}

La combinazione di stabilizzazione video (anche per compensare jitter digitale impercettibile), moving average spaziale, operazioni morfologiche e NMS si conferma fondamentale per tracciamento robusto in scenari reali, dove rumore e micromovimenti ambientali rappresentano sfide significative per approcci baseline.

\subsubsection{Risultati Quantitativi su traffico2.mp4}

Il secondo test è stato condotto sul video \texttt{traffico2.mp4} (risoluzione 1920x1080, \textbf{telecamera con micromovimenti}), utilizzando l'intero fotogramma come ROI e configurando la distanza euclidea massima a 200 pixel.

\begin{table}[H]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Pipeline} & \textbf{Oggetti Rilevati} \\
\hline
Baseline (Frame Originali) & 317 \\
Stabilized + Moving Average Spaziale & 54 \\
\hline
\textbf{Differenza (Falsi Positivi)} & \textbf{+263} \\
\hline
\end{tabular}
\caption{Confronto quantitativo detection su traffico2.mp4 (telecamera handheld)}
\label{tab:dual_pipeline_traffico2}
\end{table}

\textbf{Contesto:} A differenza di \texttt{traffico1.mp4}, questo video presenta \textbf{micromovimenti della telecamera} (handheld, piccole oscillazioni, jitter meccanico) che amplificano drasticamente i falsi positivi nella pipeline baseline. Il numero di 54 oggetti rilevati nella pipeline stabilized+filtered rimane coerente con il numero effettivo di veicoli, mentre i 317 oggetti della baseline includono 263 falsi positivi causati principalmente dal \textbf{movimento globale della scena} indotto dalla camera instabile.

\subsubsection{Analisi Visiva: Impatto Camera Shake}

La Figura \ref{fig:comparison_traffico2} mostra l'impatto drammatico del camera shake sul tracking:

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{immagini/traffico2originale.png}
    \caption{Pipeline Baseline: 317 oggetti. Micromovimenti camera causano false detection su elementi statici (edifici, cartelli, guard rail).}
    \label{fig:traffico2_baseline}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{immagini/traffico2stabilized+blur+features.png}
    \caption{Pipeline Stabilized+MA: 54 oggetti (solo veicoli). Stabilizzazione elimina movimento camera, rilevando esclusivamente oggetti effettivamente mobili.}
    \label{fig:traffico2_stabilized}
\end{subfigure}
\caption{Confronto detection traffico2.mp4: evidenza critica della stabilizzazione con camera handheld}
\label{fig:comparison_traffico2}
\end{figure}

\textbf{Osservazioni critiche (Figura \ref{fig:comparison_traffico2}):}

\begin{enumerate}
    \item \textbf{Baseline (sinistra)}: il movimento globale della scena causato dai micromovimenti della camera genera \textbf{falsi positivi massivi} su:
    \begin{itemize}
        \item Edifici e strutture statiche (appaiono in movimento relativo al frame di riferimento)
        \item Segnaletica stradale fissa (cartelli, semafori)
        \item Guard rail e barriere laterali
        \item Elementi vegetali (alberi, arbusti lungo la strada)
        \item Marciapiedi e superfici stazionarie
    \end{itemize}
    
    \item \textbf{Stabilized+MA (destra)}: la stabilizzazione feature-based \textbf{compensa completamente} il movimento camera mediante:
    \begin{itemize}
        \item Estrazione 2000 feature ORB dal frame corrente e riferimento
        \item Matching robusto con ratio test di Lowe (threshold 0.7)
        \item Stima trasformazione affine con RANSAC (outlier rejection)
        \item Warping per allineare frame corrente al riferimento fisso
    \end{itemize}
    Gli oggetti statici rimangono stabili nel frame di riferimento, venendo correttamente classificati come background. Solo i veicoli effettivamente in movimento (spostamento relativo alla strada) vengono rilevati.
    
    \item La differenza di 263 oggetti (83\% di riduzione) dimostra che \textbf{la stabilizzazione è essenziale} per scenari con camera non perfettamente stabile.
\end{enumerate}

\subsubsection{Confronto Maschere MOG2: Efficacia Stabilizzazione + Blurring}

La Figura \ref{fig:mask_traffico2} evidenzia l'impatto combinato di stabilizzazione e moving average sulle maschere MOG2:

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{immagini/traffico2rumorejitterdifferenze.png}
\caption{Confronto maschere MOG2 traffico2.mp4: originale (sinistra) vs stabilized+MA (destra). Riduzione drastica rumore globale da camera shake.}
\label{fig:mask_traffico2}
\end{figure}

\textbf{Analisi dettagliata maschere (Figura \ref{fig:mask_traffico2}):}

\begin{itemize}
    \item \textbf{Maschera Originale (sinistra)}: presenta:
    \begin{itemize}
        \item \textbf{Rumore globale diffuso}: intere regioni statiche (edifici, strada) classificate erroneamente come foreground a causa dello spostamento globale indotto da camera shake
        \item \textbf{Jitter estremo}: pixel oscillano tra foreground/background frame-by-frame, generando instabilità temporale
        \item \textbf{Contorni multipli}: stesso oggetto genera più contorni a causa delle vibrazioni
        \item \textbf{Frammentazione severa}: veicoli appaiono come insiemi disconnessi di regioni sparse
    \end{itemize}
    
    \item \textbf{Maschera Stabilized+MA (destra)}:
    \begin{itemize}
        \item \textbf{Background pulito}: elementi statici (allineati al riferimento) correttamente identificati come background, maschera quasi completamente nera nelle regioni senza movimento reale
        \item \textbf{Foreground compatto}: solo veicoli effettivamente in movimento appaiono come regioni foreground ben definite
        \item \textbf{Riduzione rumore}: moving average spaziale (kernel 5×5) elimina noise ad alta frequenza residuo post-stabilizzazione
        \item \textbf{Contorni netti}: bounding box precise grazie a maschere pulite
    \end{itemize}
    
    \item \textbf{Impatto quantitativo}:
    \begin{itemize}
        \item Riduzione detection: da 317 a 54 ($-83\%$)
        \item Eliminazione falsi positivi da shake: 263 detection spurie rimosse
        \item Qualità ROI per feature matching ORB: maschere pulite $\rightarrow$ keypoints estratte dalle vere auto, non da rumore
        \item Stabilità temporale ID: minore jitter $\rightarrow$ tracciamento coerente multi-frame
    \end{itemize}
\end{itemize}

\textbf{Conclusione traffico2.mp4:} In presenza di camera handheld (anche con micromovimenti minimi), la stabilizzazione video non è un'ottimizzazione ma una \textbf{necessità assoluta}. Senza stabilizzazione, il background subtraction fallisce completamente, identificando erroneamente l'intera scena statica come foreground in movimento. La combinazione stabilizzazione (ORB + affine transform) + moving average spaziale riduce i falsi positivi dell'83\%, rendendo il tracciamento praticamente utilizzabile.

\subsubsection{Confronto Comparativo traffico1 vs traffico2}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Video} & \textbf{Camera} & \textbf{Baseline} & \textbf{Stabilized+MA} & \textbf{Riduzione} \\
\hline
traffico1.mp4 & Fissa & 204 & 33 & -84\% \\
traffico2.mp4 & Handheld & 317 & 54 & -83\% \\
\hline
\end{tabular}
\caption{Confronto efficacia dual-pipeline su video con caratteristiche camera diverse}
\label{tab:comparison_videos}
\end{table}

Entrambi i video dimostrano efficacia costante della pipeline stabilized+filtered ($\sim$83-84\% riduzione falsi positivi), ma con cause diverse:
\begin{itemize}
    \item \textbf{traffico1}: falsi positivi da micromovimenti ambientali (erba, alberi) + jitter digitale
    \item \textbf{traffico2}: falsi positivi da camera shake (movimento globale scena) + micromovimenti ambientali
\end{itemize}

\subsection{Metriche Qualitative}

Le metriche calcolate confrontando la sequenza originale con le tre sequenze elaborate forniscono le seguenti valutazioni quantitative.

\subsubsection{Risultati traffico1.mp4 (1280x720)}

\textbf{Compressione JPEG (quality=50):}
\begin{itemize}
    \item MSE: 19.3086, PSNR: 35.28 dB, SSIM: 0.9539
\end{itemize}

\textbf{Gaussian Blurring (kernel 5×5):}
\begin{itemize}
    \item MSE: 54.7975, PSNR: 30.75 dB, SSIM: 0.9248
\end{itemize}

\textbf{Unsharp Masking (amount=1.5):}
\begin{itemize}
    \item MSE: 49.0004, PSNR: 31.23 dB, SSIM: 0.9678
\end{itemize}

\subsubsection{Risultati traffico2.mp4 (1920x1080)}

\textbf{Compressione JPEG (quality=50):}
\begin{itemize}
    \item MSE: 6.4638, PSNR: 40.09 dB, SSIM: 0.9715
\end{itemize}

\textbf{Gaussian Blurring (kernel 5×5):}
\begin{itemize}
    \item MSE: 18.2882, PSNR: 35.98 dB, SSIM: 0.9715
\end{itemize}

\textbf{Unsharp Masking (amount=1.5):}
\begin{itemize}
    \item MSE: 17.2458, PSNR: 36.17 dB, SSIM: 0.9853
\end{itemize}

\subsubsection{Analisi Metriche - traffico1.mp4}

Le metriche sono calcolate confrontando le sequenze elaborate (JPEG, Blur, Sharpen) con il video originale \texttt{traffico1.mp4} come ground truth.

\textbf{Compressione JPEG (quality=50):}
\begin{itemize}
    \item MSE: 19.3086, PSNR: 35.28 dB, SSIM: 0.9539
    \item \textbf{Interpretazione:} La compressione JPEG con quality=50 rappresenta un buon compromesso tra dimensione file e fedeltà visiva. Il PSNR di 35.28 dB indica una degradazione moderata rispetto al video originale, mentre l'SSIM di 0.9539 conferma che la struttura geometrica macroscopica è ben preservata. Gli artefatti di blocking sono presenti in regioni omogenee (cielo, asfalto) a causa della quantizzazione DCT su blocchi 8×8, ma non compromettono significativamente la percezione visiva. Il valore MSE relativamente contenuto (19.31) deriva dalla natura locale degli artefatti JPEG, che introducono errori concentrati sui bordi dei blocchi piuttosto che degradazioni diffuse sull'intero frame.
\end{itemize}

\textbf{Sfocatura Gaussiana (kernel 5×5):}
\begin{itemize}
    \item MSE: 54.7975, PSNR: 30.75 dB, SSIM: 0.9248
    \item \textbf{Interpretazione:} La sfocatura gaussiana produce l'\textbf{MSE più elevato} (54.80) tra i tre algoritmi, riflettendo la perdita diffusa di dettagli ad alta frequenza rispetto al video originale. Il kernel 5×5 introduce un effetto smoothing moderato visibile ma non estremo. L'SSIM di 0.9248 rimane alto perché la struttura geometrica (bordi, contrasti locali relativi) è preservata nonostante la riduzione di nitidezza. Il PSNR di 30.75 dB indica una degradazione percepibile ma accettabile. Il blurring gaussiano è il filtro passa-basso per eccellenza: attenua uniformemente tutte le componenti ad alta frequenza senza introdurre artefatti strutturati, causando una perdita omogenea di dettagli fini su tutto il frame.
\end{itemize}

\textbf{Unsharp Masking (amount=1.5):}
\begin{itemize}
    \item MSE: 49.0004, PSNR: 31.23 dB, SSIM: 0.9678
    \item \textbf{Interpretazione:} L'unsharp masking con amount=1.5 produce l'\textbf{SSIM più elevato} (0.9678) tra i tre algoritmi, indicando che lo sharpening preserva e potenzialmente migliora la similarità strutturale percepita rispetto all'originale. Il PSNR di 31.23 dB è superiore al blurring, confermando che l'enfatizzazione dei bordi (quando moderata) non introduce degradazioni severe. L'MSE di 49.00 deriva dalle modifiche locali ai contorni (aggiunta di componenti ad alta frequenza), ma queste alterazioni sono generalmente percepite come miglioramenti della nitidezza piuttosto che degradazioni. L'assenza di artefatti di halo eccessivi con amount=1.5 mantiene alta la qualità visiva.
\end{itemize}

\textbf{Ranking qualità percepita traffico1 (ordinamento per SSIM):}
\begin{enumerate}
    \item Unsharp Masking: SSIM 0.9678 (miglioramento percettivo nitidezza)
    \item JPEG Compression: SSIM 0.9539 (degradazione controllata moderata)
    \item Gaussian Blurring: SSIM 0.9248 (degradazione da perdita dettagli)
\end{enumerate}

\subsubsection{Analisi Metriche - traffico2.mp4}

Le metriche sono calcolate confrontando le sequenze elaborate (JPEG, Blur, Sharpen) con il video originale \texttt{traffico2.mp4} come ground truth.

\textbf{Compressione JPEG (quality=50):}
\begin{itemize}
    \item MSE: 6.4638, PSNR: 40.09 dB, SSIM: 0.9715
    \item \textbf{Interpretazione:} La compressione JPEG produce un \textbf{PSNR eccellente} (40.09 dB), indicando una qualità quasi indistinguibile rispetto al video originale alla risoluzione 1920×1080. L'MSE molto basso (6.46) conferma che gli errori introdotti dalla quantizzazione DCT sono minimali quando distribuiti su un frame ad alta risoluzione. L'SSIM di 0.9715 è elevato, dimostrando che gli artefatti di blocking tipici del JPEG (blocchi 8×8) sono poco percepibili grazie alla maggiore densità di pixel. Il quality factor 50 rappresenta un compromesso ottimale: riduzione significativa della dimensione file con degradazione visiva praticamente impercettibile.
\end{itemize}

\textbf{Sfocatura Gaussiana (kernel 5×5):}
\begin{itemize}
    \item MSE: 18.2882, PSNR: 35.98 dB, SSIM: 0.9715
    \item \textbf{Interpretazione:} Il blurring gaussiano mantiene un PSNR solido (35.98 dB) nonostante la perdita di dettagli ad alta frequenza. L'MSE di 18.29 riflette modifiche diffuse ma contenute rispetto all'originale. Notevole è l'SSIM di 0.9715, identico al JPEG, indicando che il kernel 5×5 preserva efficacemente la struttura geometrica macroscopica del video. La sfocatura attenua rumore e dettagli fini, ma non compromette bordi principali e contrasti locali. Questo dimostra che l'SSIM è meno sensibile alla perdita di componenti ad alta frequenza rispetto a MSE/PSNR, rispecchiando meglio la percezione visiva umana che tollera smoothing moderato.
\end{itemize}

\textbf{Unsharp Masking (amount=1.5):}
\begin{itemize}
    \item MSE: 17.2458, PSNR: 36.17 dB, SSIM: 0.9853
    \item \textbf{Interpretazione:} L'unsharp masking raggiunge l'\textbf{SSIM più elevato} (0.9853) tra tutte le trasformazioni, confermando che l'enfatizzazione controllata dei bordi migliora la similarità strutturale percepita rispetto all'originale. Il PSNR di 36.17 dB e l'MSE di 17.25 sono ottimi, indicando che le modifiche introdotte (aggiunta di componenti ad alta frequenza ai contorni) non costituiscono degradazione ma miglioramento percettivo. L'assenza di halo artifacts e overshooting con amount=1.5 mantiene l'immagine naturale pur aumentando la nitidezza apparente. Questo risultato evidenzia come SSIM valorizzi correttamente miglioramenti di sharpness che MSE/PSNR potrebbero classificare come errori.
\end{itemize}

\textbf{Ranking qualità percepita traffico2 (ordinamento per SSIM):}
\begin{enumerate}
    \item Unsharp Masking: SSIM 0.9853 (miglioramento percettivo nitidezza eccellente)
    \item JPEG Compression: SSIM 0.9715 (degradazione minima)
    \item Gaussian Blurring: SSIM 0.9715 (degradazione controllata, struttura preservata)
\end{enumerate}

\subsection{Conclusioni}

I risultati sperimentali dimostrano che la combinazione di stabilizzazione video e rimozione del rumore è \textbf{essenziale} per il tracciamento affidabile di oggetti in movimento. La riduzione di oltre l'83\% dei falsi positivi su entrambi i video testati conferma l'efficacia dell'approccio dual-pipeline implementato.

Il sistema sviluppato trova applicazione immediata in scenari reali quali:
\begin{itemize}
    \item \textbf{Monitoraggio del traffico}: rilevamento e conteggio automatico di veicoli per analisi dei flussi stradali
    \item \textbf{Videosorveglianza}: telecamere di sicurezza per identificazione di oggetti in movimento con riduzione drastica di allarmi falsi
\end{itemize}

Per quanto riguarda le metriche di qualità, i test hanno evidenziato che \textbf{SSIM} si rivela la metrica più affidabile per valutazioni qualitative allineate alla percezione visiva umana, distinguendo correttamente miglioramenti da degradazioni, dove MSE e PSNR mostrano limitazioni.

\end{document}